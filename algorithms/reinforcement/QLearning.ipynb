{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic Tac Toe Game implementation using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [\" \" for _ in range(9)]\n",
    "        self.current_winner = None\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in [self.board[i * 3 : (i + 1) * 3] for i in range(3)]:\n",
    "            print(\"| \" + \" | \".join(row) + \" |\")\n",
    "\n",
    "    @staticmethod\n",
    "    def print_board_nums():\n",
    "        number_board = [[str(i) for i in range(j * 3, (j + 1) * 3)] for j in range(3)]\n",
    "        for row in number_board:\n",
    "            print(\"| \" + \" | \".join(row) + \" |\")\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i for i, spot in enumerate(self.board) if spot == \" \"]\n",
    "\n",
    "    def empty_squares(self):\n",
    "        return \" \" in self.board\n",
    "\n",
    "    def num_empty_squares(self):\n",
    "        return self.board.count(\" \")\n",
    "\n",
    "    def make_move(self, square, player):\n",
    "        if self.board[square] == \" \":\n",
    "            self.board[square] = player\n",
    "            if self.winner(player):\n",
    "                self.current_winner = player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def winner(self, player):\n",
    "        board = [self.board[i * 3 : (i + 1) * 3] for i in range(3)]\n",
    "        win_conditions = [\n",
    "            [board[0][0], board[0][1], board[0][2]],\n",
    "            [board[1][0], board[1][1], board[1][2]],\n",
    "            [board[2][0], board[2][1], board[2][2]],\n",
    "            [board[0][0], board[1][0], board[2][0]],\n",
    "            [board[0][1], board[1][1], board[2][1]],\n",
    "            [board[0][2], board[1][2], board[2][2]],\n",
    "            [board[0][0], board[1][1], board[2][2]],\n",
    "            [board[0][2], board[1][1], board[2][0]],\n",
    "        ]\n",
    "        return [player, player, player] in win_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent implementation\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances. It does not require a model of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\n",
    "\n",
    "Formula for Q-Learning: $ Q(s,a) = Q(s,a) + \\alpha * (R(s) + \\gamma * max(Q(s',a')) - Q(s,a)) $\n",
    "\n",
    "Where:\n",
    "- $ Q(s,a) $ is the Q-value for state $ s $ and action $ a $.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ R(s) $ is the reward for state $ s $.\n",
    "- $ \\gamma $ is the discount factor.\n",
    "- $ s' $ is the next state.\n",
    "- $ a' $ is the next action.\n",
    "\n",
    "Algorithm:\n",
    "1. Choose an action $ a $ in the current world state $ s $ based on current Q-value estimates $ Q(s,a) $ or pick a random action with probability $ \\epsilon $.\n",
    "2. Take the action $ a $ and observe the outcome state $ s' $ and reward $ r $.\n",
    "3. Update the Q-value of current state and previous states using the formula $ Q(s,a) = Q(s,a) + \\alpha * (R(s) + \\gamma * max(Q(s',a')) - Q(s,a)) $.\n",
    "4. Set the state to the new state and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    '''\n",
    "    Q-Learning Agent\n",
    "    \n",
    "    Attributes:\n",
    "        player (str): the player's symbol ('X' or 'O')\n",
    "        alpha (float): the learning rate\n",
    "        gamma (float): the discount factor\n",
    "        epsilon (float): the exploration rate\n",
    "        q_table (dict): the Q-Table\n",
    "        path (str): the path to the Q-Table file\n",
    "\n",
    "    Methods:\n",
    "        load_q_table: Load the Q-Table from a file\n",
    "        save_q_table: Save the Q-Table to a file\n",
    "        get_state: Get the state of the board\n",
    "        get_reward: Get the reward based on the game state\n",
    "        choose_action: Choose the best action based on the Q-Table\n",
    "        update_q_values: Update the Q-Values based on the reward\n",
    "        make_action: Make an action based on the Q-Table\n",
    "        train: Train the agent\n",
    "    '''\n",
    "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.2, path=\"../../data/pickle.pkl\"):\n",
    "        self.player = player\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.path = path\n",
    "        self.q_table = self.load_q_table()\n",
    "\n",
    "    def load_q_table(self) -> dict:\n",
    "        '''Load the Q-Table from a file'''\n",
    "        try:\n",
    "            with open(self.path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "    def save_q_table(self) -> None:\n",
    "        '''Save the Q-Table to a file'''\n",
    "        with open(self.path, \"wb\") as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "\n",
    "    def get_state(self, board: str) -> str:\n",
    "        '''Get the state of the board'''\n",
    "        return \"\".join(board)\n",
    "\n",
    "    def get_reward(self, game: TicTacToe, num_moves: int) -> float:\n",
    "        '''Get the reward based on the game state'''\n",
    "        if game.current_winner == self.player:  # if the agent wins\n",
    "            return 5\n",
    "        elif game.current_winner == \"O\":        # if the opponent wins\n",
    "            return -5\n",
    "        elif not game.empty_squares():          # if it's a draw\n",
    "            return 0.5\n",
    "        elif num_moves > 2:                     # if the game is still going\n",
    "            return -0.25\n",
    "        else:\n",
    "            return 0                            # if the game just started\n",
    "\n",
    "    def choose_action(self, game: TicTacToe) -> int:\n",
    "        '''Choose the best action based on the Q-Table or randomly with epsilon probability'''\n",
    "        state = self.get_state(game.board)\n",
    "        if np.random.rand() < self.epsilon or state not in self.q_table:\n",
    "            move = random.choice(game.available_moves())\n",
    "            return move\n",
    "        else:\n",
    "            q_values = [self.q_table[state][move] for move in game.available_moves()]\n",
    "            max_q_value = max(q_values)\n",
    "            best_moves = [move for move, q_value in zip(game.available_moves(), q_values) if q_value == max_q_value]\n",
    "            move = random.choice(best_moves)\n",
    "            return move\n",
    "\n",
    "    def update_q_values(self, history: list, reward: float) -> None:\n",
    "        '''Update the Q-Values based on the reward and the history'''\n",
    "        for state, action in reversed(history):\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            self.q_table[state][action] = (1 - self.alpha) * self.q_table[state][\n",
    "                action\n",
    "            ] + self.alpha * (reward + self.gamma * np.max(self.q_table[state]))\n",
    "            reward *= self.gamma\n",
    "\n",
    "    def make_action(self, game, history: list, player: str) -> None:\n",
    "        '''Make an action based on the Q-Table and update the Q-Values based on the reward and the history'''\n",
    "        state = self.get_state(game.board)\n",
    "\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "    \n",
    "        action = self.choose_action(game)\n",
    "        game.make_move(action, player)\n",
    "        history.append((state, action))\n",
    "        reward = self.get_reward(game, len(history))\n",
    "        self.update_q_values(history, reward)\n",
    "        \n",
    "\n",
    "    def train(self, episodes: int=1000) -> None:\n",
    "        '''Train the agent'''\n",
    "        for _ in range(episodes):\n",
    "            game = TicTacToe()\n",
    "            history_player = []\n",
    "            history_opponent = []\n",
    "            turn = 'X'\n",
    "\n",
    "            while game.empty_squares():\n",
    "                if turn == self.player:\n",
    "                    self.make_action(game, history_player, self.player)\n",
    "                    turn = \"O\"\n",
    "                else:\n",
    "                    self.make_action(game, history_opponent, \"O\")\n",
    "                    turn = self.player\n",
    "            \n",
    "            if _ % 10000 == 0:\n",
    "                print(f\"Episode {_}\")\n",
    "        self.save_q_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode 10000\n",
      "Episode 20000\n",
      "Episode 30000\n",
      "Episode 40000\n",
      "Episode 50000\n",
      "Episode 60000\n",
      "Episode 70000\n",
      "Episode 80000\n",
      "Episode 90000\n",
      "Episode 100000\n",
      "Episode 110000\n",
      "Episode 120000\n",
      "Episode 130000\n",
      "Episode 140000\n",
      "Episode 150000\n",
      "Episode 160000\n",
      "Episode 170000\n",
      "Episode 180000\n",
      "Episode 190000\n",
      "Episode 200000\n",
      "Episode 210000\n",
      "Episode 220000\n",
      "Episode 230000\n",
      "Episode 240000\n",
      "Episode 250000\n",
      "Episode 260000\n",
      "Episode 270000\n",
      "Episode 280000\n",
      "Episode 290000\n",
      "Episode 300000\n",
      "Episode 310000\n",
      "Episode 320000\n",
      "Episode 330000\n",
      "Episode 340000\n",
      "Episode 350000\n",
      "Episode 360000\n",
      "Episode 370000\n",
      "Episode 380000\n",
      "Episode 390000\n",
      "Episode 400000\n",
      "Episode 410000\n",
      "Episode 420000\n",
      "Episode 430000\n",
      "Episode 440000\n",
      "Episode 450000\n",
      "Episode 460000\n",
      "Episode 470000\n",
      "Episode 480000\n",
      "Episode 490000\n",
      "Episode 500000\n",
      "Episode 510000\n",
      "Episode 520000\n",
      "Episode 530000\n",
      "Episode 540000\n",
      "Episode 550000\n",
      "Episode 560000\n",
      "Episode 570000\n",
      "Episode 580000\n",
      "Episode 590000\n",
      "Episode 600000\n",
      "Episode 610000\n",
      "Episode 620000\n",
      "Episode 630000\n",
      "Episode 640000\n",
      "Episode 650000\n",
      "Episode 660000\n",
      "Episode 670000\n",
      "Episode 680000\n",
      "Episode 690000\n",
      "Episode 700000\n",
      "Episode 710000\n",
      "Episode 720000\n",
      "Episode 730000\n",
      "Episode 740000\n",
      "Episode 750000\n",
      "Episode 760000\n",
      "Episode 770000\n",
      "Episode 780000\n",
      "Episode 790000\n",
      "Episode 800000\n",
      "Episode 810000\n",
      "Episode 820000\n",
      "Episode 830000\n",
      "Episode 840000\n",
      "Episode 850000\n",
      "Episode 860000\n",
      "Episode 870000\n",
      "Episode 880000\n",
      "Episode 890000\n",
      "Episode 900000\n",
      "Episode 910000\n",
      "Episode 920000\n",
      "Episode 930000\n",
      "Episode 940000\n",
      "Episode 950000\n",
      "Episode 960000\n",
      "Episode 970000\n",
      "Episode 980000\n",
      "Episode 990000\n"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(player='X')\n",
    "agent.train(episodes=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Tic Tac Toe Game with Q-Learning Agent vs Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(player='X', path=\"../../data/pickle.pkl\")\n",
    "\n",
    "def play_game(agent: QLearningAgent):\n",
    "    game = TicTacToe()\n",
    "    turn = 'X'\n",
    "    while game.empty_squares():\n",
    "        if turn == 'X':\n",
    "            action = agent.choose_action(game)\n",
    "            game.make_move(action, 'X')\n",
    "            turn = 'O'\n",
    "        else:\n",
    "            move = random.choice(game.available_moves())\n",
    "            print(f\"Player O chooses position {move}\")\n",
    "            game.make_move(move, 'O')\n",
    "            turn = 'X'\n",
    "        game.print_board()\n",
    "        print('\\n')\n",
    "        if game.current_winner:\n",
    "            print(f\"Player {'AI' if game.current_winner == 'X' else 'O'} wins!\")\n",
    "            return 'AI' if game.current_winner == 'X' else 'O'\n",
    "    if not game.current_winner:\n",
    "        print(\"It's a tie!\")\n",
    "        return 'Tie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Values: [20.725331279416537, 20.959500600410813, 20.313741283863347, 20.873837921869473, 20.866353564549293, 20.2446856553271, 20.577835441167327, 20.834094394100575, 21.187976110677614]\n",
      "Max Q-Value: 21.187976110677614\n",
      "AI choose 8 with Q-Value 21.187976110677614\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "Player O chooses position 3\n",
      "|   |   |   |\n",
      "| O |   |   |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "AI choose 7 randomly\n",
      "|   |   |   |\n",
      "| O |   |   |\n",
      "|   | X | X |\n",
      "\n",
      "\n",
      "Player O chooses position 6\n",
      "|   |   |   |\n",
      "| O |   |   |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "AI choose 5 randomly\n",
      "|   |   |   |\n",
      "| O |   | X |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "Player O chooses position 1\n",
      "|   | O |   |\n",
      "| O |   | X |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "Q-Values: [15.466663247414818, 8.737352668998899, 9.274737496223356]\n",
      "Max Q-Value: 15.466663247414818\n",
      "AI choose 0 with Q-Value 15.466663247414818\n",
      "| X | O |   |\n",
      "| O |   | X |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "Player O chooses position 2\n",
      "| X | O | O |\n",
      "| O |   | X |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "Q-Values: [48.631106471794524]\n",
      "Max Q-Value: 48.631106471794524\n",
      "AI choose 4 with Q-Value 48.631106471794524\n",
      "| X | O | O |\n",
      "| O | X | X |\n",
      "| O | X | X |\n",
      "\n",
      "\n",
      "Player AI wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI': 95, 'O': 4, 'Tie': 1}\n"
     ]
    }
   ],
   "source": [
    "count = {'AI': 0, 'O': 0, 'Tie': 0}\n",
    "\n",
    "for _ in range(100):\n",
    "    winner = play_game(agent)\n",
    "    count[winner] += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Tic Tac Toe Game with Q-Learning Agent vs Minimax Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from search.minimax import minimax\n",
    "\n",
    "agent = QLearningAgent(player='X', path='../../data/pickle.pkl')\n",
    "\n",
    "def play_game_vs_minmax(agent: QLearningAgent):\n",
    "    game = TicTacToe()\n",
    "    turn = 'X'\n",
    "    while game.empty_squares():\n",
    "        if turn == 'X':\n",
    "            action = agent.choose_action(game)\n",
    "            game.make_move(action, 'X')\n",
    "            turn = 'O'\n",
    "        else:\n",
    "            move_info = minimax(game, 'O')\n",
    "            move = move_info['position']\n",
    "            print(f\"MinMiax O chooses position {move}\")\n",
    "            game.make_move(move, 'O')\n",
    "            turn = 'X'\n",
    "        game.print_board()\n",
    "        print('\\n')\n",
    "        if game.current_winner:\n",
    "            print(f\"Player {'Minmax' if game.current_winner == 'O' else 'AI'} wins!\")\n",
    "            return 'Minimax' if game.current_winner == 'O' else 'AI'\n",
    "    if not game.current_winner:\n",
    "        print(\"It's a tie!\")\n",
    "        return 'Tie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Values: [20.725331279416537, 20.959500600410813, 20.313741283863347, 20.873837921869473, 20.866353564549293, 20.2446856553271, 20.577835441167327, 20.834094394100575, 21.187976110677614]\n",
      "Max Q-Value: 21.187976110677614\n",
      "AI choose 8 with Q-Value 21.187976110677614\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "=====================================\n",
      "All possible moves and their scores:\n",
      "Move: 0, Score: -3\n",
      "Move: 1, Score: -3\n",
      "Move: 2, Score: -3\n",
      "Move: 3, Score: -3\n",
      "Move: 4, Score: 0\n",
      "Move: 5, Score: -3\n",
      "Move: 6, Score: -3\n",
      "Move: 7, Score: -3\n",
      "=====================================\n",
      "\n",
      "MinMiax O chooses position 4\n",
      "|   |   |   |\n",
      "|   | O |   |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "AI choose 2 randomly\n",
      "|   |   | X |\n",
      "|   | O |   |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "=====================================\n",
      "All possible moves and their scores:\n",
      "Move: 0, Score: -5\n",
      "Move: 1, Score: -5\n",
      "Move: 3, Score: -5\n",
      "Move: 5, Score: 0\n",
      "Move: 6, Score: -5\n",
      "Move: 7, Score: -5\n",
      "=====================================\n",
      "\n",
      "MinMiax O chooses position 5\n",
      "|   |   | X |\n",
      "|   | O | O |\n",
      "|   |   | X |\n",
      "\n",
      "\n",
      "AI choose 6 randomly\n",
      "|   |   | X |\n",
      "|   | O | O |\n",
      "| X |   | X |\n",
      "\n",
      "\n",
      "=====================================\n",
      "All possible moves and their scores:\n",
      "Move: 0, Score: -3\n",
      "Move: 1, Score: -3\n",
      "Move: 3, Score: 4\n",
      "Move: 7, Score: 2\n",
      "=====================================\n",
      "\n",
      "MinMiax O chooses position 3\n",
      "|   |   | X |\n",
      "| O | O | O |\n",
      "| X |   | X |\n",
      "\n",
      "\n",
      "Player Minmax wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game_vs_minmax(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI': 0, 'Minimax': 94, 'Tie': 6}\n"
     ]
    }
   ],
   "source": [
    "count = {'AI': 0, 'Minimax': 0, 'Tie': 0}\n",
    "\n",
    "for _ in range(100):\n",
    "    winner = play_game_vs_minmax(agent)\n",
    "    count[winner] += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Minimax performed much better than reinforcement learning (Q-Learning), this is only because there are not many possible moves in the game of tic-tac-toe, and brute-force algorithms perform much better. In games like Go, Q-Learning would be the favorite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
