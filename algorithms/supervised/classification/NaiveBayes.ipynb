{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes algorithm\n",
    "\n",
    "Naive Bayes is a simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features. It is easy to build and particularly useful for very large data sets. Along with other linear classifiers, Naive Bayes is a good baseline classifier to which more sophisticated methods can be compared.\n",
    "\n",
    "The Naive Bayes classifier is highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. In particular, the parameters of the model are estimated using a \"closed-form\" expression, which takes linear time, rather than iterative approximation as used for many other types of classifiers. This makes it particularly useful for very large data sets. The model is also relatively robust, requiring only a small number of training data to estimate the necessary parameters.\n",
    "\n",
    "To classify a new object we'll use the following Naive Bayes formula:\n",
    "\n",
    "\n",
    "$P(C_k|x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n|C_k)P(C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(C_k|x_1, x_2, ..., x_n): \\text{posterior probability - probability of class given features}$\n",
    "- $C_k: \\text{class}$\n",
    "- $x_i: \\text{feature}$\n",
    "- $P(x_i|C_k): \\text{likelihood - probability of feature given class}$\n",
    "- $P(C_k): \\text{prior probability - probability of class}$\n",
    "- $P(x_i): \\text{marginal probability - probability of feature (it can be ignored)}$\n",
    "\n",
    "<img src=\"../../../assets/naivebayes_example.png\" style=\"display: block; margin-left: auto; margin-right: auto\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "We have 2 classes: spam and not spam (ham) and 2 features: contains word \"free\" and contains word \"money\", we want to classify new email, which class it belongs to (spam or ham) and what is the probability of this class.\n",
    "\n",
    "|      | Spam | Ham  | Total|\n",
    "|------|------|------| -----|\n",
    "| Free | 2    | 20   | 22   |\n",
    "| Money| 5    | 10   | 15   |\n",
    "| Total| 10   | 90   | 100  |\n",
    "\n",
    "1) $Priors:\\;P(Spam) = \\frac{10}{100} = 0.1,\\;P(Ham) = \\frac{90}{100} = 0.9$\n",
    "2) $Likelihood-money:\\;P(Free|Spam) = \\frac{2}{10} = 0.2,\\;P(Free|Ham) = \\frac{20}{90} = 0.22$\n",
    "3) $Likelihood-free:\\;P(Money|Spam) = \\frac{5}{10} = 0.5,\\;P(Money|Ham) = \\frac{10}{90} = 0.11$\n",
    "4) $Posterior:\\;P(Spam|Free, Money) = P(Free, Money|Spam)P(Spam) = 0.2 \\cdot 0.5 \\cdot 0.1 = 0.01$\n",
    "5) $Posterior:\\;P(Ham|Free, Money) = P(Free, Money|Ham)P(Ham) = 0.22 \\cdot 0.11 \\cdot 0.9 = 0.02$\n",
    "6) $P(Ham|Free, Money) > P(Spam|Free, Money)$ (so we classify new email as Ham with probability 0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    '''\n",
    "    Naive Bayes classifier for text classification\n",
    "    \n",
    "    Attributes:\n",
    "        labels (list): List of unique labels\n",
    "        likelihoods (dict): Dictionary of likelihoods for each class\n",
    "        priors (dict): Dictionary of priors for each class\n",
    "        stop_words (list): List of stop words to remove from the text\n",
    "    \n",
    "    Methods:\n",
    "        _preprocess_text(text: str) -> list: Preprocess a text by removing punctuation, numbers, and stop words and returning a list of words\n",
    "        _likelihoods(X: np.ndarray, y: np.ndarray) -> dict: Calculate the likelihoods of each word in each class\n",
    "        fit(X: np.ndarray, y: np.ndarray) -> None: Fit the model to the data\n",
    "        predict(text: str) -> str: Predict the class of a text\n",
    "        evaluate(X: list, y: list) -> float: Evaluate the model on a list of texts and labels\n",
    "        show_word_probabilities(word: str) -> None: Print the likelihoods of a word in each class\n",
    "    '''\n",
    "    def __init__(self, stop_words=[]):\n",
    "        self.labels = []\n",
    "        self.likelihoods = {}\n",
    "        self.priors = {}\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Preprocess a text by removing punctuation, numbers, and stop words and returning a list of words\n",
    "\n",
    "        Args:\n",
    "            text (float): text to preprocess\n",
    "\n",
    "        Returns: list: List of preprocessed words\n",
    "        \"\"\" \n",
    "        text = str(text)\n",
    "        text = text.encode('ascii', errors='replace').decode('ascii')\n",
    "        text = text.lower()\n",
    "        text = ''.join([c for c in text if c not in '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])\n",
    "        text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "        return text.split()\n",
    "    \n",
    "    def _likelihoods(self, X: np.ndarray, y: np.ndarray) -> dict:\n",
    "        '''\n",
    "        Calculate the likelihoods of each word in each class\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): List of texts\n",
    "            y (np.ndarray): List of labels\n",
    "        \n",
    "        Returns: dict: Dictionary of likehoods for each class'''\n",
    "        # Initialize the dictionaries for the words and their counts and the likelihoods\n",
    "        dicts = {c: {} for c in self.labels}\n",
    "        likelihoods = {c: {} for c in self.labels}\n",
    "\n",
    "        # Fill the dictionaries with the words and their counts\n",
    "        for i in range(len(X)):\n",
    "            words = self._preprocess_text_(X[i])\n",
    "            for word in words:\n",
    "                if word in dicts[y[i]]:\n",
    "                    dicts[y[i]][word] += 1\n",
    "                else:\n",
    "                    dicts[y[i]][word] = 1\n",
    "        \n",
    "        # Calculate the likelihoods of each word in each class\n",
    "        for c in self.labels:\n",
    "            for word in dicts[c]:\n",
    "                likelihoods[c][word] = dicts[c][word] / sum(dicts[c].values())\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        '''\n",
    "        Fit the model to the data\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): List of texts\n",
    "            y (np.ndarray): List of labels\n",
    "        '''\n",
    "        # Get the unique labels\n",
    "        self.labels = np.unique(y)\n",
    "\n",
    "        # Calculate the priors and likelihoods\n",
    "        self.priors = {c: np.sum(c == y) /len(y) for c in self.labels}\n",
    "        self.likelihoods = self._likelihoods_(X, y)\n",
    "\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        '''\n",
    "        Predict the class of a text\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to predict the class of\n",
    "\n",
    "        Returns: str: Predicted class\n",
    "        '''\n",
    "        # Preprocess the text\n",
    "        words = self._preprocess_text_(text)\n",
    "        # Initialize the scores for each class\n",
    "        scores = {c: self.priors[c] for c in self.labels}\n",
    "        \n",
    "        for c in self.labels:\n",
    "            for word in words:\n",
    "                # If the word is in the likelihoods for the class\n",
    "                if word in self.likelihoods[c]:\n",
    "                    # Multiply the class score by the likelihoods of the word in the class\n",
    "                    scores[c] = scores[c] * self.likelihoods[c][word]\n",
    "                # If the word is NOT in the likelihood for the class\n",
    "                else:\n",
    "                    # Set the class score to 0\n",
    "                    scores[c] = 0\n",
    "\n",
    "        # Return the class with the highest score\n",
    "        return max(scores, key=scores.get)\n",
    "    \n",
    "    def evaluate(self, X: list, y: list) -> float:\n",
    "        '''\n",
    "        Evaluate the model on a list of texts and labels\n",
    "        \n",
    "        Args:\n",
    "            X (list): List of texts\n",
    "            y (list): List of labels\n",
    "        \n",
    "        Returns: float: Accuracy of the model\n",
    "        '''\n",
    "        correct = 0\n",
    "        for i in range(len(X)):\n",
    "            if self.predict(X[i]) == y[i]:\n",
    "                correct += 1\n",
    "        return correct / len(y)\n",
    "    \n",
    "    def show_word_probabilities(self, word: str) -> None:\n",
    "        '''\n",
    "        Print the likelihoods of a word in each class\n",
    "\n",
    "        Args:\n",
    "            word (str): Word to print the likelihoods of\n",
    "        '''\n",
    "        probabilities = {}\n",
    "\n",
    "        print(f'Word: {word}')\n",
    "        for c in self.labels:\n",
    "            print(f'P({word}|{c}) = {self.likelihoods[c][word]} * {self.priors[c]} / {sum(self.priors.values())} = {self.likelihoods[c][word] * self.priors[c] / sum(self.priors.values())}')\n",
    "            probabilities[c] = self.likelihoods[c][word] * self.priors[c] / sum(self.priors.values())\n",
    "\n",
    "        print(f'Prediction: {max(probabilities, key=probabilities.get)}')\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        for c in self.labels:\n",
    "            print(f'Class: {c}')\n",
    "            print(f'Prior: {self.priors[c]}')\n",
    "            print(f'Likelihoods: {self.likelihoods[c]}')\n",
    "\n",
    "        return ''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the [SMS Spam Collection Dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI Machine Learning Repository. This dataset contains 5574 SMS messages that have been collected for mobile phone spam research. The collection is composed by just one text file, where each line has the correct class followed by the raw message. We will use this dataset to train a Naive Bayes classifier that can classify new messages as spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../../../data/SMSSpamCollection.csv', sep='\\t', names=['label', 'message'])\n",
    "stop_words = pd.read_csv('../../../data/stop_words_EN.txt', delimiter='t', on_bad_lines='skip')\n",
    "stop_words = np.array(stop_words)\n",
    "\n",
    "X = np.array(data.message)\n",
    "y = np.array(data.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracies: 0.9294708520179372\n",
      "Standard deviation: 0.007950782665996496\n",
      "Minimum accuracies: 0.9139013452914798\n",
      "Maximum accuracies: 0.9461883408071748\n"
     ]
    }
   ],
   "source": [
    "n_tests = 50 # Number of tests to run\n",
    "accuracies = [] # List to store the accuracies\n",
    "\n",
    "model = NaiveBayes(stop_words=stop_words)\n",
    "\n",
    "for i in range(n_tests):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracies.append(model.evaluate(X_test, y_test))\n",
    "\n",
    "print(f'Average accuracies: {np.mean(accuracies)}') # Average score is the mean of the scores => formula: sum(x) / n\n",
    "print(f'Standard deviation: {np.std(accuracies)}')  # Standard deviation is a measure of how spread out numbers are => formula: sqrt(sum((x - mean(x))**2) / n)\n",
    "print(f'Minimum accuracies: {np.min(accuracies)}')\n",
    "print(f'Maximum accuracies: {np.max(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different words and their frequency in spam and ham messages (probability and class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: free\n",
      "P(free|ham) = 0.0009218150349913462 * 0.866502131478573 / 1.0 = 0.0007987546926489968\n",
      "P(free|spam) = 0.014084507042253521 * 0.13349786852142698 / 1.0 = 0.001880251669315873\n",
      "Prediction: spam\n",
      "---\n",
      "Word: call\n",
      "P(call|ham) = 0.003612009933027316 * 0.866502131478573 / 1.0 = 0.003129814305889947\n",
      "P(call|spam) = 0.02207627789207153 * 0.13349786852142698 / 1.0 = 0.00294713604347825\n",
      "Prediction: ham\n",
      "---\n",
      "Word: text\n",
      "P(text|ham) = 0.0010158777936639326 * 0.866502131478573 / 1.0 = 0.0008802602735315477\n",
      "P(text|spam) = 0.008308276626048425 * 0.13349786852142698 / 1.0 = 0.0011091372206638575\n",
      "Prediction: spam\n",
      "---\n",
      "Word: you\n",
      "P(you|ham) = 0.027673263601474905 * 0.866502131478573 / 1.0 = 0.02397894189564642\n",
      "P(you|spam) = 0.019069473017882577 * 0.13349786852142698 / 1.0 = 0.0025457340017141874\n",
      "Prediction: ham\n",
      "---\n",
      "Word: stop\n",
      "P(stop|ham) = 0.0006208142072390699 * 0.866502131478573 / 1.0 = 0.0005379368338248346\n",
      "P(stop|spam) = 0.007675265073587593 * 0.13349786852142698 / 1.0 = 0.001024631527660897\n",
      "Prediction: spam\n",
      "---\n",
      "Word: reply\n",
      "P(reply|ham) = 0.0006584393107081045 * 0.866502131478573 / 1.0 = 0.0005705390661778549\n",
      "P(reply|spam) = 0.006488368412723532 * 0.13349786852142698 / 1.0 = 0.0008661833532803459\n",
      "Prediction: spam\n",
      "---\n",
      "Word: mobile\n",
      "P(mobile|ham) = 0.00026337572428324177 * 0.866502131478573 / 1.0 = 0.00022821562647114195\n",
      "P(mobile|spam) = 0.007596138629529989 * 0.13349786852142698 / 1.0 = 0.0010140683160355269\n",
      "Prediction: spam\n",
      "---\n",
      "Word: service\n",
      "P(service|ham) = 7.525020693806908e-05 * 0.866502131478573 / 1.0 = 6.520446470604057e-05\n",
      "P(service|spam) = 0.0034024370944769743 * 0.13349786852142698 / 1.0 = 0.00045421809989091313\n",
      "Prediction: spam\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "words = ['free', 'call', 'text', 'you', 'stop', 'reply', 'mobile', 'service']\n",
    "\n",
    "for word in words:\n",
    "    model.show_word_probabilities(word)\n",
    "    print('---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "This [SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) is public available at the [UCI Machine Learning Repository](https://archive-beta.ics.uci.edu/dataset/228/sms+spam+collection) for research. \n",
    "\n",
    "Tiago A. Almeida and José María Gómez Hidalgo\\\n",
    "Department of Computer Science\\\n",
    "Federal University of Sao Carlos (UFSCar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
