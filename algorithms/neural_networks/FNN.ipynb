{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FNN:\n",
    "    '''\n",
    "    A simple feedforward neural network implementation using numpy.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size (int): Number of input features\n",
    "        hidden_size (int): Number of hidden units\n",
    "        output_size (int): Number of output units\n",
    "        learning_rate (float): Learning rate for the model\n",
    "        epochs (int): Number of epochs for training\n",
    "        activation (str): Activation function for the hidden layer\n",
    "\n",
    "        weights_input_hidden (ndarray): Weights between input and hidden layer\n",
    "        weights_hidden_output (ndarray): Weights between hidden and output layer\n",
    "        bias_hidden (ndarray): Bias for the hidden layer\n",
    "        bias_output (ndarray): Bias for the output layer\n",
    "        activation (function): Activation function for the hidden layer\n",
    "        activation_derivative (function): Derivative of the activation function\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        learning_rate: float = 0.01,\n",
    "        epochs: int = 1000,\n",
    "        activation: str = \"sigmoid\",\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.activation_derivative = lambda x: x * (1 - x)\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = lambda x: np.maximum(0, x)\n",
    "            self.activation_derivative = lambda x: np.where(x >= 0, 1, 0)\n",
    "        elif activation == \"softmax\":\n",
    "            self.activation = lambda x: np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "            self.activation_derivative = lambda x: x * (1 - x)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Activation function not supported. Please use 'sigmoid', 'relu', or 'softmax'.\"\n",
    "            )\n",
    "\n",
    "    def _forward_pass(self, X: np.ndarray) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform the forward pass.\n",
    "\n",
    "        Args:\n",
    "            X: Input features numpy array\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing hidden layer output and final predicted output\n",
    "        \"\"\"\n",
    "        hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_layer_output = self.activation(hidden_layer_input)\n",
    "        output_layer_input = (\n",
    "            np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        )\n",
    "        predicted_output = self.activation(output_layer_input)\n",
    "        return hidden_layer_output, predicted_output\n",
    "\n",
    "    def _backward_pass(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        hidden_layer_output: np.ndarray,\n",
    "        predicted_output: np.ndarray,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform the backward pass and update weights and biases.\n",
    "\n",
    "        Args:\n",
    "            X: Input features numpy array\n",
    "            y: Target numpy array\n",
    "            hidden_layer_output: Output from the hidden layer\n",
    "            predicted_output: Final predicted output\n",
    "        \"\"\"\n",
    "        output_error = y - predicted_output\n",
    "        output_delta = output_error * self.activation_derivative(predicted_output)\n",
    "\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.activation_derivative(hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.learning_rate * np.dot(\n",
    "            hidden_layer_output.T, output_delta\n",
    "        )\n",
    "        self.bias_output += self.learning_rate * np.sum(\n",
    "            output_delta, axis=0, keepdims=True\n",
    "        )\n",
    "        self.weights_input_hidden += self.learning_rate * np.dot(X.T, hidden_delta)\n",
    "        self.bias_hidden += self.learning_rate * np.sum(\n",
    "            hidden_delta, axis=0, keepdims=True\n",
    "        )\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Train the model using the given numpy arrays X and y.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of input features\n",
    "            y: numpy array of target values\n",
    "        \"\"\"\n",
    "        for _ in range(self.epochs):\n",
    "            hidden_layer_output, predicted_output = self._forward_pass(X)\n",
    "            self._backward_pass(X, y, hidden_layer_output, predicted_output)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict output for the given input features.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of input features\n",
    "\n",
    "        Returns:\n",
    "            numpy array of predicted values\n",
    "        \"\"\"\n",
    "        _, predicted_output = self._forward_pass(X)\n",
    "        return predicted_output\n",
    "\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, metric: str = \"mse\") -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model performance using the given metric.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of input features\n",
    "            y: numpy array of target values\n",
    "            metric: Evaluation metric (\"mse\", \"mae\", \"r2_score\", or \"accuracy\")\n",
    "\n",
    "        Returns:\n",
    "            Evaluation result as a float\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        if metric == \"mse\":\n",
    "            return np.mean((predictions - y) ** 2)\n",
    "        elif metric == \"mae\":\n",
    "            return np.mean(np.abs(predictions - y))\n",
    "        elif metric == \"r2_score\":\n",
    "            return 1 - (np.sum((y - predictions) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "        elif metric == \"accuracy\":\n",
    "            y_pred = np.argmax(predictions, axis=1)\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "            return np.mean(y_pred == y_true)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid metric\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        String representation of the FNN class.\n",
    "\n",
    "        Returns:\n",
    "            String description of the model\n",
    "        \"\"\"\n",
    "        return f\"FNN: input_size={self.input_size}, hidden_size={self.hidden_size}, output_size={self.output_size}, learning_rate={self.learning_rate}, epochs={self.epochs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Neural Network on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X = X / X.max()\n",
    "y = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "fnn = FNN(\n",
    "    input_size=64,\n",
    "    hidden_size=128,\n",
    "    output_size=10,\n",
    "    learning_rate=0.01,\n",
    "    epochs=1000,\n",
    "    activation=\"softmax\",\n",
    ")\n",
    "\n",
    "fnn.fit(X_train, y_train)\n",
    "print(f'Accuracy: {fnn.evaluate(X_test, y_test, metric=\"accuracy\"):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
